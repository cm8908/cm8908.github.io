---
layout: default
title: "Knowledge Bank"
author: Minseop Jung
date: Since March 2024
abstract: 각종 CS/DS/Algo/ML/DL/AI/Data Science 지식들을 간결 명료하게 저장합니다. 
---
<p class="Section">1. &ensp; Data Structure / Algorithm</p>
<p class="BodyText">

</p>

<!-- ----------------------------------------------------------------- -->
<p class="Section">2. &ensp;Artificial Intelligence</p>
<p class="SubSection">Deep Learning</p>
<p class="BodyText">
    <b>CNN (Convolutional Neural Network)</b>란 무엇인가? 이미지 데이터를 학습하는 데 주로 사용되는 특징 추출자(Feature extractor)다. Convolution 연산을 주요 구성 요소로 가지며, pooling이나 배치 정규화 레이어 등을 포함하기도 한다. 2차원의 경우, \(k \times k\)의 convolution kernel(혹은 filter)를 데이터에 대해 sliding하며, kernel의 가중치들과 데이터의 값들 (이미지 데이터의 경우 pixel value)과 element-wise product를 수행한 뒤, aggregate한다. 이러한 계산 과정을 반복하면 데이터의 지역적 특징들이 aggreagte된 feature map을 얻을 수 있다.<br>
    <b>Convolution 연산의 출력 차원은 어떻게 계산되는가?</b> \(\frac{W - k + 2*P}{S} + 1\) 이 때, W는 데이터의 width, k는 kernel size, P는 padding size, S는 stride이다.<br>
    <b>왜 CNN은 이미지 데이터에 효과적인가?</b> 이미지 데이터는 지역적 정보가 중요한 데이터이다. 예를 들어, 사람 얼굴 이미지라고 하면 눈, 코, 입, 귀 등등의 semantic한 정보는 인접한 픽셀들이 담고 있다. 따라서 이러한 지역적 정보를 convolution을 통해 추출하는 CNN이 이미지 데이터의 특징 추출에 효과적이다.<br>
    <b>Fully Convolutional Neural Network</b>는 convolution & deconvolution 연산으로만 구성된 CNN이다. 기존 CNN과 달리 fully-connected layer가 들어가지 않기 때문에, 입력 사이즈를 보존해야 하는 segmentation 태스크 등에서 유리하다.<br>
    <b>CapsuleNet</b>은 CNN의 max pooling 연산이 가지는 translation invariance를 지적하며 제안되었다. Max pooling 과정에서 위치 정보가 손실될 수 있고, convolution 연산은 태생적으로 이미지 내 객체의 위치에 무관하게 학습되기 때문에 전역적 정보가 충분히 보존되지 않을 수 있다. Convolution의 수용 영역 한계는 <b>ViT</b>의 attention mechanism으로도 극복할 수 있다.
</p>
<p class="BodyText">
    <b>RNN (Recurrent Neural Network)</b>이란 무엇인가? RNN은 여러 time-step에 걸쳐서, 현재 time-step에서 신경망의 출력이 다음 time-step의 신경망의 입력으로 들어가는 구조를 띄고 있다. 이러한 구조는 태생적으로 순서가 중요한 자연어 데이터, 시계열 데이터 등의 시퀀스 데이터의 특징을 추출하는데 유리하다.<br>
    <b>LSTM (Long-Short Term Memory)</b>이란 무엇인가? LSTM은 RNN의 한 종류인데, vanilla RNN에 비해 더 복잡한 구조를 지닌다. RNN과 달리 장기기억 메모리와 단기기억 메모리 두 개의 path로 hidden state가 time-step을 걸쳐 전달된다.<br>
    <b>RNN의 한계는 무엇인가?</b> Long-term dependency와 gradient vanishing 문제가 있다. Long-term dependency는 시퀀스의 길이가 길어질 수록, RNN이 시퀀스 앞쪽의 내용을 까먹는 문제이다. Gradient vanishing도 비슷하게 시퀀스 길이가 길어질수록, 역전파 때 gradient가 앞쪽까지 전달되지 못하는 문제이다. Gradient가 1보다 작은 값으로 계산될 때, chain rule에 의해 gradient가 곱해지면서 출력 레이어에서 멀어질수록 0에 가까운 값이 계산되기 때문이다. 이를 완화하기 위해 LSTM이 고안되었으나 순환 구조의 구조적 문제 때문에 완전히 해결하지는 못한다.<br>
    <b>Attention mechanism이란 무엇인가?</b> Query, key, value 세 가지 input에 대해 query와 key의 유사도(i.e. attention score)를 계산하고, 이를 다시 value값과 내적하는 방식으로 계산되는 특징 추출 방식이다. 위와 같은 RNN의 문제를 완화하고자 제안되었는데, 매 RNN time-step마다 현재 step의 RNN hidden state를 query, 이전 step들의 RNN hidden state들을 key, value로 사용하여 attend하면 time-step이 길어지더라도 이전 값들을 참조하기 때문에 long-term dependency와 gradient vanishing 문제를 해결할 수 있다.<br>
    <b>Transformer란 무엇인가?</b> Attention mechanism으로만 구성된 신경망 구조이다. 기본적으로 기계번역을 위한 encoder-decoder 구조로 제안되었으나, encoder나 decoder만 사용하여 다양한 태스크에 적용된다. Self-attention mechanism을 새롭게 제안하여 입력 시퀀스 내에서의 장거리 종속성 학습을 유리하게 만들었으며, 효율적인 병렬처리가 가능하게 했다. <br>
</p>

<p class="SubSection">Machine Learning</p>
<p class="SubSection">Reinforcement Learning</p>
<p class="BodyText">

</p>
<!-- ----------------------------------------------------------------- -->

<p class="Section">3. &ensp;Data Science / Mining / Analysis</p>
<p class="BodyText">
    <b>독립변수</b>란 무엇인가? 다른 변수에 영향을 주는 "원인"이 되는 변수로, \(y=ax+b\)에서 \(x\)에 해당한다. <b>종속변수</b>란 무엇인가? 다른 변수에 영향을 받는 "결과"가 되는 변수로, \(y=ax+b\)에서 \(y\)에 해당한다.
</p>
<p class="BodyText">
    <b>가설 검성</b>이란 무엇인가? 통계적 기법으로 가설의 타당성을 검증하는 방법이다. 어떤 결과가 통계적으로 유의미한 결과인지, 아니면 단순 샘플링 에러에 불과한지를 판단한다. 이는 <b>귀무 가설</b>과 <b>대립 가설</b>을 세우고, 표본 데이터를 통해 귀무 가설을 채택할지 기각할 지를 통해 결정한다. 이 때 귀무 가설은 "A와 B가 차이가 없다"는 가설이고, 대립 가설은 "차이가 있다"는 가설이다. 귀무 가설을 거짓으로 판단하고 기각하면, A와 B는 통계적으로 유의미한 차이가 있다고 말할 수 있다. 이 때 t-test나 ANOVA-test와 같은 방법을 사용하여 표본 데이터를 통해 기각과 채택을 판단한다.
</p>
<p class="BodyText">
    <b>Type I Error</b>와 <b>Type II Error</b>는 무엇인가? 위의 귀무 가설이 참인데 (즉 차이가 없는데) 차이가 있다고 판단하여 기각하는 것이 Type I Error고, 반대로 귀무 가설이 거짓인데 (즉 차이가 있는데) 차이가 없다고 판단하여 채택하는 것이 Type II Error이다. 
</p>
<p class="BodyText">
    <b>p-value</b>란 무엇인가? 귀무 가설이 참일 확률을 나타낸다. 즉 p-value가 낮으면 귀무 가설이 거짓일 확률이 높으므로 통계적으로 유의미한 결과라고 판단할 수 있다. 일반적으로 p-value가 0.05보다 낮으면 유의미한 결과라고 판단한다.
</p>
<p class="BodyText">
    왜 귀무가설은 <b>부정</b>의 형태로 설정되는가? "차이가 있다"보다 "차이가 없다"가 검증하기 쉽기 때문이다. 차이가 있다는 말은 너무 모호하고 광범위하다. 또한, 과학은 신중하기 때문에 새로운 가설에 대해서는 항상 회의적이다.
</p>
<p class="BodyText">
    <b>데이터 마이닝의 5단계 프로세스.</b><br>
    <b>1단계, 문제 정의 Problem Definition.</b> 해결하고자 하는 문제를 (구체적, 과학적으로) 정의한다. 예를 들어, "중환자실 산소호흡기에서 이상이 발생하는 문제를 해결하고 싶다." \(\rightarrow\) "중환자실 산소 호흡기에서 수집된 breath data 중에서 normal과 abnormal 데이터를 분류하고자 한다."<br>
    <b>2단계, 데이터 수집 및 전처리 Data Collection & Preprocessing.</b> 문제 해결을 위해 필요한 데이터를 수집하고, 데이터의 품질을 보장하기 위해 결측치 처리, 노이즈 제거, 불완전한 데이터 제거 등 데이터를 전처리하는 과정이다.<br>
    <b>3단계, 데이터 표현 Data Representation.</b> 수집된 데이터를 데이터 마이닝 알고리즘 적용에 적합한 형태로 변환하는 과정이다. 이 때, 정규화, 이산화, 특징 공학 등이 사용될 수 있다. 경우에 따라서는 raw data가 사용되기도 한다.<br>
    <b>4단계, DM Function/Techniques.</b> 머신러닝, 통계분석, 시계열분석 등... 질적, 양적 분석 모두 이루어지며, 둘은 상호보완적이다.<br>
    <b>5단계, Evaluation & Interpretation.</b> 분석 결과의 신뢰성과 유용성을 평가하고, 분석 결과로부터 결론 (지식)을 도출한다.
</p>
<p class="BodyText">
    <b>Curse of Dimensionality 차원의 저주</b>란 무엇인가? 다루고자 하는 데이터의 차원이 커짐에 따라 발생하는 문제로, 차원이 커짐에 따라 데이터 공간의 밀도가 기하급수적으로 낮아지고, 이에 따라 기계학습이나 데이터 분석(e.g. clustering)에 필요한 데이터가 희소해지는 현상을 말한다. 또한, 차원이 높아질수록 처리하기 위해 필요한 연산 복잡도가 기하급수적으로 증가하게 된다.
</p>
<p class="BodyText">
    <b>Dimensionality Reduction 차원 축소</b> 방법은 무엇이 있는가? PCA, Attribute Subset Selection 등이 있다.
</p>
<p class="BodyText">
    <b>Principal Component Analysis (PCA)</b>란 무엇인가? 데이터의 분산을 최대한 보존하면서 데이터의 차원을 축소하는 방법으로, 데이터의 차원이 n이라면, n보다 작은 개수의 직교 벡터(주성분 벡터)를 찾아 이들로 데이터를 표현하는 방법이다.
</p>
<p class="BodyText">
    <b>Attribute Subset Selection</b>이란 무엇인가? 중복되거나(redundant) 무관한(irrelevant) 특징을 제거하는 방법이다. 이 때, 각 특징에 대해 통계 검정을 적용하여 각 특징들을 단계적으로 제거하거나 추가할 수 있다.
</p>
<p class="BodyText">
    <b>Numerosity Reduction</b>이란 무엇인가? 데이터를 "양"을 줄이는 방법이다. 여기에는 <i>parametric</i> 방법과 <i>non-parametric</i> 방법이 있다. Parametric 방법은 데이터를 잘 표현하는 모델(e.g. regression model)을 찾아 모델의 파라미터로 데이터를 표현하는 방법이다. Non-parametric 방법은 데이터의 대표값을 찾아 데이터를 표현하는 방법이다. 예를 들어, clustering, histogram, sampling 등이 있다.
</p>
<p class="BodyText">
    <b>Data Compression</b>은 무엇인가? 데이터를 압축하는 방법으로, 데이터의 종류에 따라 다양한 알고리즘이 존재한다. Lossless 방법은 정보의 손실 없이 압축하는 방법이며 (e.g. string), lossy 방법은 정보의 손실을 어느정도 감수하는 압축 방법이다 (e.g. audio/video).
</p>
<p class="BodyText">
    통계 분석에서 <b>parametric</b> 방법과 <b>non-parametric</b> 방법은 무엇인가? <b>Parametric</b> 방법은 데이터를 잘 표현하는 몇 개의 매개 변수(파라미터)로 데이터를 표현하는 모델을 구성한다. 이 때, 데이터가 특정 분포를 따른다는 가정이 필요하다. <b>Non-parametric</b> 방법은 파라미터를 따로 설정하지 않고 데이터 자체만으로 모델을 구성한다. 이 때, 분포에 대한 가정은 필요치 않으나 많은 데이터가 필요하다.
</p>
<p class="BodyText">
    데이터 시각화에서 <b>어떤 색깔을 선택하는 것이 좋을까?</b> 첫쨰, <b>Qualitative</b> color는 서로 잘 구분되는 "범주형 데이터"를 나타낼 때 유용하다. 둘째, <b>Sequential</b> color는 증가하거나 감소하는 순서가 있는 데이터를 나타낼 때 유용하다. 셋째, <b>Diverging</b> color는 중간값에서 양극으로 감소하고 증가하는 데이터를 나타낼 때 유용하다.
</p>
<p class="BodyText">
    <b>상관계수 Correlation coefficient</b>이란 무엇인가? 두 변수 사이의 상관관계를 나타내는 지표이다. 두 변수 사이의 양의 상관관계가 높다면, 두 변수의 단조 관계 비슷하다 (e.g. 하나가 감소할 떄, 나머지 하나도 감소한다). 음의 상관관계가 높다면, 두 변수의 단조 관계가 반대이다 (e.g. 하나 감소하면 나머지 증가). 상관계수가 0에 가깝다면, 두 변수의 단조 관계가 관련이 없다. 상관계수는 Pearson's correlation과 Spearman's correlation이 대표적이다.
</p>
<p class="BodyText">
    <b>Pearson's correlation</b> coefficient는 두 변수의 공분산을 이용해 계산되기 때문에, 데이터가 정규 분포를 따른다는 가정이 필요하다. 반면 <b>Spearman's correlation</b> coefficient는 변수의 rank(순서)에 기반하여 계산하기 때문에, 정규 분포 가정이 필요 없다. 두 지표 모두 \(\left[-1, 1\right]\)의 값을 가진다.
</p>
<p class="BodyText">
    Graph 이론에서, 노드의 <b>Centrality</b> (or Prestige)란 무엇인가? 노드의 중심성 즉, 노드가 전체 그래프에서 다른 노드에 비해 얼마나 더 중요한 지를 나타낸다. Degree centrality, closeness centrality, betweenness centrality, eigenvector centrality가 있다.<br>
    <b>Degree centrality</b>란 degree(이웃 노드의 개수)가 높은 노드를 중요한 노드로 본다.<br>
    <b>Closeness centrality</b>란 다른 노드와의 평균 거리가 가장 가까운 (또는 가중치가 가장 높은) 노드를 중요한 노드로 본다.<br>
    <b>Betweenness centrality</b>란 다른 노드들과 많이 연결해주는 노드를 중요한 노드로 본다.<br>
    <b>Eigenvector centrality</b>란 중요한 노드와 연결된 노드를 중요한 노드로 본다.<br>
</p>
<p class="BodyText">
    자연어 처리에서 텍스트를 나타내는 방법이 무엇이 있는가?<br>
    먼저, <b>Bag of Words</b>은 텍스트 문서에 등장하는 각 단어들의 등장 빈도를 벡터로 나타낸 것이다. 그러나 vocabulary의 사이즈가 커질수록 희소 벡터가 만들어지고, 단어의 등장 순서나 문맥을 고려하지 않으며, 빈도에 집중하기 때문에 의미 없는 단어(a, the, is 등)에 높은 가중치를 준다.<br>
    <b>TF-IDF</b>는 term frequency (문서 내에서 단어의 등장 빈도)와 inverse document frequency (document frequence - 특정 단어가 전체 문서중에서 몇개의 문서에서 등장하는지 - 의 역수)의 곱으로 계산된다. 일반적으로 흔히 등장하는 의미 없는 단어들에 대해 낮은 가중치를 주고, 특정 문서에 자주 등장하는 (진짜 중요한) 단어에 가중치를 높게 준다. 그러나 여전히 문맥은 무시하는 단점이 있다.<br>
    <b>N-gram</b>은 문서 내의 N개의 연속된 단어들을 시퀀스로 묶어서 표현하는 방법이다. 예를 들어, "I love you"에 대한 Bi-gram은 ("I love", "love you")이다. N-gram은 문맥을 고려할 수 있으나, N이 커질수록 발생하는 차원의 저주를 피할 수 없다.<br>
    <b>Word Embedding</b>은 주로 학습된 딥러닝 모델을 통해 각각의 단어를 임베딩 벡터로 나타내는 방법이다. 각각의 임베딩 벡터들은 임베딩 공간에서 단어들 간의 의미적 관계를 반영하여 표현된다.
</p>
<p class="BodyText">
    시계열 데이터의 구성 요소는 무엇이 있는가? 첫째, <b>추세 Trend</b>는 시간에 따른 데이터가 증가하거나 감소하는 경향을 나타낸다. 둘째, <b>계절성 Seasonality</b>는 주로 계절이나 기후와 같이 절대적 시간 주기에 따라 반복되는 정보를 담고 있으며 비교적 짧은 순환 주기를 가진다. Naive한 딥러닝 방법으로 예측하지 못한다. 셋째, <b>주기성 Cycle</b>은 불규칙적인 시간 간격에 따라 반복되는 정보를 담고 있으며, 비교적 장기적인 순환 주기를 가진다. 넷째, <b>불규칙성 Irregularity</b>은 외부적인 사건에 의해서 발생하는 예측할 수 없는 무작위적인 성분이다.
</p>    


<p class="IndexSection">Index</p>
<p class="IndexBtn" fontsize>
    <a href="/notes/notes-index.html">Back to Notes-Index</a> <br>
    <a href="/index.html">Back to Home</a>
</p>   