---
layout: default
title: "Shuffling BN - Issues with BN in Contrastive Learning Setup"
author: Minseop Jung
date: 2024-06-07
abstract: MoCo의 저자들은 contrastive learning setup에서 batch normalization의 문제점을 지적하여 Shuffling BN을 제안했다. BN에는 어떤 문제가 있고, Shuffling BN은 어떻게 그 문제를 해결했을까?
---
<p class="Section">Issues with Vanilla BN in Contrastive Learning</p>
<p class="BodyText">
    Information Leakage / Cheating: CL에서, 배치 내에는 positive sample과 negative sample이 혼재해있다. 그런데 배치 정규화 시 배치 내의 sample들에 대해 mean과 variance를 계산하기 때문에, positive와 negative sample들이 "shared information"을 학습할 수 있다. 이로 인해 모델이 의미 있는 표현 학습을 수행하는 대신, 이러한 shared information을 활용해 positive-negative를 구별해 단순히 loss만 줄이는 학습을 수행할 수 있다.
</p>
<p class="Section">How does Shuffling BN Works?</p>
<p class="BodyText">
    Shuffling BN은 이러한 문제를 예방하고자, Multi-GPU training에서 각 GPU에 올라와있는 intermediate image feature들을 섞어버린다. 이렇게 섞인 image features에 대해 batch normalization statistics를 계산하기 때문에, 기존 minibatch간의 shared information이 발생하는 것을 막아버린다.<br>
     비유를 들자면, 시험을 볼때마다 옆자리 짝꿍을 무작위로 바꿔버려서 서로 상의해서 cheating하는일이 없도록 하는 것과 같은 이치인듯 하다.
</p>