---
layout: default
title: A Lightweight CNN-Transformer Model for Learning Traveling Salesman Problems
author: Minseop Jung, Jaeseung Lee, Jiibum Kim
date: "Preprint uploaded on May 2023 (Updated on March 2024)"
abstract: Several studies have attempted to solve traveling salesman problems (TSPs) using various deep learning techniques. Among them, Transformer-based models show state-of-the-art performance even for large-scale Traveling Salesman Problems (TSPs). However, they are based on fully-connected attention models and suffer from large computational complexity and GPU memory usage. Our work is the first CNN-Transformer model based on a CNN embedding layer and partial self-attention for TSP. Our CNN-Transformer model is able to better learn spatial features from input data using a CNN embedding layer compared with the standard Transformer-based models. It also removes considerable redundancy in fully-connected attention models using the proposed partial self-attention. Experimental results show that the proposed CNN embedding layer and partial self-attention are very effective in improving performance and computational complexity. The proposed model exhibits the best performance in real-world datasets and outperforms other existing state-of-the-art (SOTA) Transformer-based models in various aspects.
---
<p class="Section">Status</p>

<p class="BodyText">
    Preprint was first uploaded at <a href="https://arxiv.org/abs/2305.01883">arXiv:2305.01883</a> on May 2023. Revised version was uploaded on March 2024. Submission for journal publication is still under review.
</p>

<p class="Section">Implementation</p>

<p class="BodyText">
    Implemented using python3 along with PyTorch and numpy. Source code available on GitHub: <a href="https://github.com/cm8908/CNN_Transformer3">https://github.com/cm8908/CNN_Transformer3</a>
</p>

<p class="IndexSection">Index</p>
<p class="IndexBtn" fontsize>
    <a href="/index.html">Back to Home</a>
</p>   